{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/urgpa/OneDrive/Escritorio/Unity/PuppetTest/Assets/_Sonidos/jammin.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\urgpa\\anaconda3\\lib\\site-packages\\librosa\\filters.py:238: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn(\n",
      "<ipython-input-9-113b7f0357f1>:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_noises['times'] = df_noises['times'].apply(lambda x: x[0])\n",
      "<ipython-input-9-113b7f0357f1>:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_noises_hip_rot['times'] = df_noises['times'].apply(lambda x: x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SONG PROCESSED AND SAVED\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import json\n",
    "import math\n",
    "import statistics\n",
    "import librosa.display\n",
    "import scipy as sp\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from tkinter import Tk     # from tkinter import Tk for Python 3.x\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "#OPEN FILE\n",
    "Tk().withdraw() # we don't want a full GUI, so keep the root window from appearing\n",
    "filename = askopenfilename() # show an \"Open\" dialog box and return the path to the selected file\n",
    "file_dir = filename\n",
    "print(file_dir)\n",
    "#LOAD SONG\n",
    "signal, samplerate = librosa.load(file_dir)\n",
    "#GET GENERAL TEMPO\n",
    "signal_bpm, samplerate_bpm = librosa.load(file_dir, duration = 60, sr = 10000)\n",
    "onset_env = librosa.onset.onset_strength(signal_bpm, sr=samplerate_bpm)\n",
    "tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=samplerate_bpm)\n",
    "#DIVIDE INTO COMPASES\n",
    "song_duration= librosa.get_duration(y=signal, sr=samplerate) #duration of the song\n",
    "beat_num = 1\n",
    "bpm = tempo\n",
    "vcompas_time = beat_num * 60 / bpm #duration of a compas                                       \n",
    "vcompas_num  = song_duration / vcompas_time                  #number of compases \n",
    "compas_sample_num = vcompas_time * samplerate                #number of samples on each compas\n",
    "song_sample_num = compas_sample_num * vcompas_num            #total number of samples in the song\n",
    "vcompas_data = {} \n",
    "for vcompas_i in range(0,int(vcompas_num)):                                \n",
    "    start = math.floor(compas_sample_num * vcompas_i)          #start sample for the compas\n",
    "    end = math.floor(compas_sample_num * (vcompas_i + 1))      #end sample for the compas\n",
    "    selected_samples = signal[start:end]                       #array with all the compas samples\n",
    "    vcompas_data[vcompas_i] = ([selected_samples],vcompas_time * vcompas_i)\n",
    "df_compas = pd.DataFrame.from_dict(vcompas_data, orient = 'index', columns = ['wform','times'])\n",
    "#signals\n",
    "df_means = df_compas\n",
    "df_means['max'] = df_means.wform.apply(lambda x: x[0].max())\n",
    "\n",
    "max_signal = max(list(df_means['max']))\n",
    "min_signal = min(list(df_means['max']))\n",
    "signal_umbral = max_signal - min_signal\n",
    "\n",
    "max_noise = 0.8\n",
    "min_noise = 0.35\n",
    "noise_umbral = max_noise - min_noise\n",
    "\n",
    "df_means['gnre'] = df_means['max'].apply(lambda x: (x * noise_umbral / signal_umbral) + min_noise)\n",
    "\n",
    "df_noises = df_means[['times','gnre']]\n",
    "df_noises['times'] = df_noises['times'].apply(lambda x: x[0])\n",
    "\n",
    "df_means = df_compas\n",
    "df_means['max'] = df_means.wform.apply(lambda x: x[0].max())\n",
    "\n",
    "max_signal = max(list(df_means['max']))\n",
    "min_signal = min(list(df_means['max']))\n",
    "signal_umbral = max_signal - min_signal\n",
    "\n",
    "max_noise = 1.2\n",
    "min_noise = 0\n",
    "noise_umbral = max_noise - min_noise\n",
    "\n",
    "df_means['gnre'] = df_means['max'].apply(lambda x: round(((x * noise_umbral / signal_umbral) + min_noise), 4))\n",
    "\n",
    "df_noises_hip_rot = df_means[['times','gnre']]\n",
    "df_noises_hip_rot['times'] = df_noises['times'].apply(lambda x: x)\n",
    "\n",
    "\n",
    "#CREATE A DATA FRAME WITH THE FREQUENCIES ARRAY\n",
    "df_freq = pd.DataFrame(columns = ['mag']) \n",
    "for csignal in df_compas['wform']:\n",
    "    csignal = csignal[0]\n",
    "    X = np.fft.fft(csignal)\n",
    "    X_mag = np.absolute(X)\n",
    "    #print(X_mag[0])\n",
    "    newdf = pd.DataFrame([[X_mag]], columns = ['mag'])\n",
    "    df_freq = pd.concat([df_freq, newdf], axis = 0)\n",
    "\n",
    "df_freq = df_freq.reset_index(drop = True)\n",
    "#GET MAXIMUS OF THE FREQUENCIES\n",
    "instrument_division = [0,100,300,600,1000,2500,7000,15000]\n",
    "fixed_instrument_div = list(instrument_division)\n",
    "fixed_instrument_div.remove(0)\n",
    "\n",
    "f = np.linspace(0, samplerate, len(X_mag), endpoint = False)\n",
    "df_bin_freq = pd.DataFrame(columns = fixed_instrument_div)\n",
    "\n",
    "for ampl in df_freq.mag: #loop time domain compases\n",
    "    \n",
    "    freq_full = [] #list with al freq list\n",
    "\n",
    "    for i in range(1,len(instrument_division)): #loop through freq domain\n",
    "        \n",
    "        start = instrument_division[i - 1] * len(ampl) / f[len(ampl) - 2] #starting point of the frequency bin\n",
    "        end = instrument_division[i] * len(ampl) / f[len(ampl) - 2] #end point\n",
    "        \n",
    "        freq_bin = ampl[math.floor(start):math.floor(end)] #save magintudes bin\n",
    "        freq_full += [freq_bin]\n",
    "        #print(freq_bin)\n",
    "        #dat_tuple = (list(ampl).index(ampl[start:end].max()),signl[start:end].max())\n",
    "        #print(f[dat_tuple[0]], dat_tuple)\n",
    "\n",
    "    newdf_cluster_freq = pd.DataFrame([freq_full], columns = fixed_instrument_div)\n",
    "    df_bin_freq = pd.concat([df_bin_freq, newdf_cluster_freq])\n",
    "    \n",
    "df_bin_freq = df_bin_freq.reset_index()\n",
    "df_bin_freq.drop(['index'], inplace = True, axis = 1)\n",
    "#LOWER\n",
    "df_lower = df_bin_freq.drop(columns = [600,1000,2500,7000,15000])\n",
    "df_lower.columns = list(map(lambda x: str(x), df_lower.columns))\n",
    "df_lower['100_mean'] = df_lower['100'].apply(lambda x: x.mean())\n",
    "df_lower['100_median'] = df_lower['100'].apply(lambda x: statistics.median(x))\n",
    "df_lower['100_max'] = df_lower['100'].apply(lambda x: x.max())\n",
    "df_lower['300_mean'] = df_lower['300'].apply(lambda x: x.mean())\n",
    "df_lower['300_median'] = df_lower['300'].apply(lambda x: statistics.median(x))\n",
    "df_lower['300_max'] = df_lower['300'].apply(lambda x: x.max())\n",
    "df_lower['noise'] = df_means['gnre']\n",
    "df_lower_model = df_lower.drop(['100','300'], axis = 1)\n",
    "scaler = StandardScaler().fit(df_lower_model)\n",
    "X_prep = scaler.transform(df_lower_model)\n",
    "kmeans = GaussianMixture(n_components= 2, covariance_type='diag',max_iter =  100,n_init = 8, random_state = 1234)\n",
    "kmeans.fit(X_prep)\n",
    "yhat = kmeans.predict(X_prep)\n",
    "clusters = np.unique(yhat)\n",
    "df_low_final = df_compas.drop(columns = ['wform'])\n",
    "df_low_final['times'] = df_low_final['times'].apply(lambda x: x[0])\n",
    "df_low_final['gnre'] = yhat\n",
    "#MID\n",
    "df_mid = df_bin_freq.drop(columns = [100,300,2500,7000,15000])\n",
    "df_mid.columns = list(map(lambda x: str(x), df_mid.columns))\n",
    "df_mid['600_mean'] = df_mid['600'].apply(lambda x: x.mean())\n",
    "df_mid['600_median'] = df_mid['600'].apply(lambda x: statistics.median(x))\n",
    "df_mid['600_max'] = df_mid['600'].apply(lambda x: x.max())\n",
    "df_mid['1000_mean'] = df_mid['1000'].apply(lambda x: x.mean())\n",
    "df_mid['1000_median'] = df_mid['1000'].apply(lambda x: statistics.median(x))\n",
    "df_mid['1000_max'] = df_mid['1000'].apply(lambda x: x.max())\n",
    "df_mid['noise'] = df_means['gnre']\n",
    "df_mid_model = df_mid.drop(['600','1000'], axis = 1)\n",
    "scaler = StandardScaler().fit(df_mid_model)\n",
    "X_prep = scaler.transform(df_mid_model)\n",
    "kmeans = GaussianMixture(n_components= 2, covariance_type='spherical',max_iter = 300,n_init = 8, random_state = 1234, init_params = 'random')\n",
    "kmeans.fit(X_prep)\n",
    "yhat = kmeans.predict(X_prep)\n",
    "clusters = np.unique(yhat)\n",
    "df_mid_final = df_compas.drop(columns = ['wform'])\n",
    "df_mid_final['times'] = df_mid_final['times'].apply(lambda x: x[0])\n",
    "df_mid_final['gnre'] = yhat\n",
    "#HIGH\n",
    "df_high = df_bin_freq.drop(columns = [100,300,600,1000])\n",
    "df_high.columns = list(map(lambda x: str(x), df_high.columns))\n",
    "df_high['2500_mean'] = df_high['2500'].apply(lambda x: x.mean())\n",
    "df_high['2500_median'] = df_high['2500'].apply(lambda x: statistics.median(x))\n",
    "df_high['2500_max'] = df_high['2500'].apply(lambda x: x.max())\n",
    "df_high['7000_mean'] = df_high['7000'].apply(lambda x: x.mean())\n",
    "df_high['7000_median'] = df_high['7000'].apply(lambda x: statistics.median(x))\n",
    "df_high['7000_max'] = df_high['7000'].apply(lambda x: x.max())\n",
    "df_high['15000_mean'] = df_high['15000'].apply(lambda x: x.mean())\n",
    "df_high['15000_median'] = df_high['15000'].apply(lambda x: statistics.median(x))\n",
    "df_high['15000_max'] = df_high['15000'].apply(lambda x: x.max())\n",
    "df_high['noise'] = df_means['gnre']\n",
    "df_high_model = df_high.drop(['2500','7000','15000'], axis = 1)\n",
    "scaler = StandardScaler().fit(df_mid_model)\n",
    "X_prep = scaler.transform(df_mid_model)\n",
    "kmeans = GaussianMixture(n_components= 4, covariance_type='spherical',max_iter = 100,n_init = 20, random_state = 1234, init_params = 'kmeans')\n",
    "kmeans.fit(X_prep)\n",
    "yhat = kmeans.predict(X_prep)\n",
    "clusters = np.unique(yhat)\n",
    "df_high_final = df_compas.drop(columns = ['wform'])\n",
    "df_high_final['times'] = df_high_final['times'].apply(lambda x: x[0])\n",
    "df_high_final['gnre'] = yhat\n",
    "#FULL\n",
    "full_freq = pd.concat([df_high_model, df_mid_model, df_lower_model], axis = 1)\n",
    "scaler = StandardScaler().fit(full_freq)\n",
    "X_prep = scaler.transform(full_freq)\n",
    "kmeans = KMeans(n_clusters= 3,\n",
    "               init=\"random\",\n",
    "               n_init= 2,  # try with 1, 4, 8, 20, 30, 100...\n",
    "               max_iter= 300,\n",
    "               tol= 1e-4,\n",
    "               algorithm=\"auto\",\n",
    "               random_state=1234)\n",
    "kmeans.fit(X_prep)\n",
    "yhat = kmeans.predict(X_prep)\n",
    "clusters = np.unique(yhat)\n",
    "df_full_final = df_compas.drop(columns = ['wform'])\n",
    "df_full_final['times'] = df_full_final['times'].apply(lambda x: x[0])\n",
    "df_full_final['gnre'] = yhat\n",
    "#FEATURES\n",
    "step_freq = 1/(vcompas_time * 2)\n",
    "noise_freq = 1/(vcompas_time * 2)\n",
    "#SAVING\n",
    "dict_full = {}\n",
    "dict_low = {}\n",
    "dict_mid = {}\n",
    "dict_high = {}\n",
    "dict_legs = {}\n",
    "dict_noises = {}\n",
    "dict_noises_hand = {} \n",
    "\n",
    "for i in range(0,len(df_low_final.times)):\n",
    "    dict_low[str(df_low_final.times[i])] = str(df_low_final.gnre[i])\n",
    "    dict_mid[str(df_mid_final.times[i])] = str(df_mid_final.gnre[i])\n",
    "    dict_high[str(df_high_final.times[i])] = str(df_high_final.gnre[i])\n",
    "    dict_legs[str(df_full_final.times[i])] = str(df_full_final.gnre[i])\n",
    "    dict_noises[str(df_noises.times[i])] = str(df_noises.gnre[i])\n",
    "    dict_noises_hand[str(df_noises_hip_rot.times[i])] = str(df_noises_hip_rot.gnre[i])\n",
    "    \n",
    "dict_full['low'] = str(dict_low)\n",
    "dict_full['mid'] = str(dict_mid)\n",
    "dict_full['high'] = str(dict_high)\n",
    "dict_full['legs'] = str(dict_legs)\n",
    "dict_full['noises'] = str(dict_noises)\n",
    "dict_full['hnoises'] = str(dict_noises_hand)\n",
    "dict_full['freqs'] = str(step_freq[0])\n",
    "json_dump = json.dumps(dict_full)\n",
    "with open('../json/poses.json', 'w') as f:\n",
    "    f.write(json_dump)\n",
    "with open(r'C:\\Users\\urgpa\\OneDrive\\Escritorio\\Unity\\PuppetTest\\Assets\\_Scripts\\Poses\\poses.json', 'w') as f:\n",
    "    f.write(json_dump)\n",
    "    \n",
    "print('SONG PROCESSED AND SAVED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
